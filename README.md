# cifar10_QAT

> **Quantizationâ€‘Aware Training (QAT)** of neural networks on the **CIFARâ€‘10 dataset** â€” enabling compact and efficient models suitable for deployment on lowâ€‘resource devices.

---

## ğŸ“Œ Overview

This repository implements **Quantizationâ€‘Aware Training (QAT)** for image classification models on the CIFARâ€‘10 dataset.
QAT simulates fixedâ€‘point quantization of weights and activations during training to help models maintain accuracy after quantization and deployment on hardware with limited precision.

---

## ğŸš€ Features

âœ” Support for training and evaluating models with QAT
âœ” YAML configuration for flexible experiment setup
âœ” Example scripts for training baseline and quantized models
âœ” Modular project structure for models, quantization helpers, and utilities

---

## ğŸ“ Repository Structure

```plaintext
.
â”œâ”€â”€ docs/                         # Documentation (usage, concepts)
â”œâ”€â”€ examples/                     # Example scripts (e.g., LSQ quantization)
â”œâ”€â”€ model/                        # Model definitions
â”œâ”€â”€ quan/                         # QAT / quantization modules
â”œâ”€â”€ util/                         # Utility scripts (data loading, metrics)
â”œâ”€â”€ main.py                       # Training entrypoint
â”œâ”€â”€ main_analytical.py            # Analytical experiments
â”œâ”€â”€ config.yaml                   # Default configuration
â”œâ”€â”€ set_cifar10*.yaml             # CIFARâ€‘10 specific configs
â”œâ”€â”€ requirements.txt              # Dependencies
â””â”€â”€ README.md                     # This file
```

---

## ğŸ“¦ Installation
1. **Fix interactive shell:**

   ```bash
   exec bash
   ```

2. **Clone the repository:**

   ```bash
   git clone https://github.com/ChrisShakkour/cifar10_QAT.git
   cd cifar10_QAT
   ```

3. **Set up a Python environment:**

   ```bash
   python -m venv venv
   source venv/bin/activate  # macOS/Linux
   venv\Scripts\activate     # Windows
   ```
4. **Or Set up a miniconda3 environment:**

   ```bash
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   bash Miniconda3-latest-Linux-x86_64.sh
   source ~/.bashrc
   conda create --name venv
   ```
   
5. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ§  Usage

### ğŸ”¹ validation of a Baseline Model in 32-bit

```bash
python main.py set_cifar10_baseline.yaml
```
### ğŸ”¹ training of a pre-trained Baseline Model in 32-bit

```bash
python main.py set_cifar10_baseline_training.yaml
```

### ğŸ”¹ Train with Quantizationâ€‘Aware Training (QAT)

```bash
python main.py set_cifar10.yaml
```

Configuration files control training hyperparameters such as learning rate, batch size, number of epochs, and quantization settings.

---

## ğŸ“Š Evaluation & Results

During and after training, logs, evaluation metrics, and model checkpoints are saved to the output directory specified in the configuration files.

You can compare:

* Fullâ€‘precision (baseline) accuracy
* Quantizationâ€‘aware training accuracy
* Accuracy vs. model size / deployment efficiency tradeâ€‘offs

*(Add quantitative results, plots, or tables here if available.)*

---

## ğŸ§° Configuration

All experiments are configured using YAML files. Example configuration:

```yaml
see set_cifar10.yaml
```

Adjust parameters according to your experiment needs.

---

## ğŸ“š About CIFARâ€‘10

The **CIFARâ€‘10** dataset consists of **60,000 32Ã—32 color images** across **10 classes**:

* airplane
* automobile
* bird
* cat
* deer
* dog
* frog
* horse
* ship
* truck

It is split into **50,000 training images** and **10,000 test images** and is commonly used for benchmarking image classification models.

---

## ğŸ§ª Dependencies

Key dependencies include:

* Python 3.x
* PyTorch (or the deepâ€‘learning framework used in this repository)
* PyYAML
* NumPy
* Additional packages listed in `requirements.txt`

---

## ğŸ› ï¸ Contributing

Contributions are welcome. Feel free to open an issue or submit a pull request for improvements, bug fixes, or new features.

---

## ğŸ“„ License

This project is released under the **MIT License**. See the `LICENSE` file for details.

---

## ğŸ™ Acknowledgements

This project builds on ideas from the quantization and efficient deepâ€‘learning research community. Thanks to all openâ€‘source contributors whose work made this project possible.
